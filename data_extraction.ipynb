{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ceb7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yaml import safe_load\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d193b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for file in os.listdir('archive (1)/t20s/'):\n",
    "    filenames.append(os.path.join('archive (1)/t20s/',file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3404d430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archive (1)/t20s/1001349.yaml',\n",
       " 'archive (1)/t20s/1001351.yaml',\n",
       " 'archive (1)/t20s/1001353.yaml',\n",
       " 'archive (1)/t20s/1004729.yaml',\n",
       " 'archive (1)/t20s/1007655.yaml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filenames[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62edd1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 4570\n",
      "First 5: ['archive (1)/t20s/1001349.yaml', 'archive (1)/t20s/1001351.yaml', 'archive (1)/t20s/1001353.yaml', 'archive (1)/t20s/1004729.yaml', 'archive (1)/t20s/1007655.yaml']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 4570/4570 [09:32<00:00,  7.98file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done.\n",
      "✅ Total processed files: 0 / 4570\n",
      "⚠️ Errors in 0 files.\n",
      "Final DataFrame shape: (0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "data_folder = 'archive (1)/t20s/'\n",
    "\n",
    "# Collect YAML files only\n",
    "filenames = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.yaml')]\n",
    "print(f\"Total files found: {len(filenames)}\")\n",
    "print(\"First 5:\", filenames[:5])\n",
    "\n",
    "# Shared dictionary to collect results from multiple processes\n",
    "manager = multiprocessing.Manager()\n",
    "results = manager.dict()\n",
    "error_files = manager.list()\n",
    "\n",
    "# Function to load YAML file safely\n",
    "def load_yaml_file(file_path, idx, results_dict, error_list):\n",
    "    try:\n",
    "        # Skip very large files (optional, set your limit)\n",
    "        if os.path.getsize(file_path) > 10_000_000:  # 10 MB limit\n",
    "            error_list.append((file_path, \"File too large\"))\n",
    "            return\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "\n",
    "        if not isinstance(data, dict):\n",
    "            error_list.append((file_path, \"Not a dict\"))\n",
    "            return\n",
    "\n",
    "        # Flatten YAML and add match_id\n",
    "        df = pd.json_normalize(data)\n",
    "        df['match_id'] = idx + 1\n",
    "\n",
    "        results_dict[idx] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        error_list.append((file_path, str(e)))\n",
    "\n",
    "# Start processing\n",
    "start_time = time.time()\n",
    "final_df_list = []\n",
    "\n",
    "for idx, file in enumerate(tqdm(filenames, desc=\"Processing files\", unit=\"file\")):\n",
    "    p = multiprocessing.Process(target=load_yaml_file, args=(file, idx, results, error_files))\n",
    "    p.start()\n",
    "    p.join(timeout=5)  # 5 seconds timeout per file\n",
    "\n",
    "    if p.is_alive():\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "        error_files.append((file, \"Timeout\"))\n",
    "\n",
    "    # Optional: stop total processing after some time\n",
    "    if time.time() - start_time > 600:  # 10 minutes\n",
    "        print(\"⏳ Total timeout reached, stopping early...\")\n",
    "        break\n",
    "\n",
    "# Collect all DataFrames\n",
    "final_df_list = [results[i] for i in sorted(results.keys())]\n",
    "\n",
    "# Combine results\n",
    "if final_df_list:\n",
    "    final_df = pd.concat(final_df_list, ignore_index=True)\n",
    "else:\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "# Backup copy\n",
    "backup = final_df.copy()\n",
    "\n",
    "print(\"\\n✅ Done.\")\n",
    "print(f\"✅ Total processed files: {len(final_df_list)} / {len(filenames)}\")\n",
    "print(f\"⚠️ Errors in {len(error_files)} files.\")\n",
    "print(\"Final DataFrame shape:\", final_df.shape)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43b6c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches left after filtering. Check your filters (male, T20, 20 overs).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ===============================\n",
    "# STEP 0: Make sure 'final_df' exists\n",
    "# ===============================\n",
    "\n",
    "# ===============================\n",
    "# STEP 1: Backup original DataFrame\n",
    "# ===============================\n",
    "backup = final_df.copy()\n",
    "\n",
    "# ===============================\n",
    "# STEP 2: Keep only male matches\n",
    "# ===============================\n",
    "if 'info.gender' in final_df.columns:\n",
    "    final_df = final_df[final_df['info.gender'] == 'male']\n",
    "    final_df.drop(columns=['info.gender'], inplace=True)\n",
    "\n",
    "# ===============================\n",
    "# STEP 3: Keep only T20 matches with 20 overs\n",
    "# ===============================\n",
    "if 'info.match_type' in final_df.columns:\n",
    "    final_df = final_df[final_df['info.match_type'] == 'T20']\n",
    "\n",
    "if 'info.overs' in final_df.columns:\n",
    "    final_df = final_df[final_df['info.overs'] == 20]\n",
    "\n",
    "# ===============================\n",
    "# STEP 4: Drop unnecessary columns safely\n",
    "# ===============================\n",
    "columns_to_drop = [\n",
    "    'info.overs', 'info.match_type',\n",
    "    'meta.data_version', 'meta.created', 'meta.revision',\n",
    "    'info.outcome.bowl_out', 'info.bowl_out',\n",
    "    'info.supersubs.South Africa', 'info.supersubs.New Zealand',\n",
    "    'info.outcome.eliminator', 'info.outcome.result', 'info.outcome.method',\n",
    "    'info.neutral_venue', 'info.match_type_number',\n",
    "    'info.outcome.by.runs', 'info.outcome.by.wickets'\n",
    "]\n",
    "\n",
    "final_df.drop(columns=[c for c in columns_to_drop if c in final_df.columns], inplace=True)\n",
    "\n",
    "# ===============================\n",
    "# STEP 5: Save cleaned DataFrame\n",
    "# ===============================\n",
    "pickle.dump(final_df, open('dataset_level1.pkl', 'wb'))\n",
    "\n",
    "# ===============================\n",
    "# STEP 6: Access first match's first innings deliveries safely\n",
    "# ===============================\n",
    "matches = pickle.load(open('dataset_level1.pkl', 'rb'))\n",
    "\n",
    "if not matches.empty:\n",
    "    first_match = matches.iloc[0]\n",
    "    first_match_innings = first_match.get('innings', None)\n",
    "\n",
    "    if first_match_innings and isinstance(first_match_innings, list):\n",
    "        try:\n",
    "            first_innings = first_match_innings[0].get('1st innings', {})\n",
    "            deliveries = first_innings.get('deliveries', [])\n",
    "            print(deliveries)\n",
    "        except Exception as e:\n",
    "            print(\"Error accessing deliveries:\", e)\n",
    "    else:\n",
    "        print(\"No innings data found in the first match.\")\n",
    "else:\n",
    "    print(\"No matches left after filtering. Check your filters (male, T20, 20 overs).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9d34e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matches: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Original matches:\", len(backup))\n",
    "\n",
    "# Male filter\n",
    "if 'info.gender' in backup.columns:\n",
    "    male_df = backup[backup['info.gender'] == 'male']\n",
    "    print(\"After male filter:\", len(male_df))\n",
    "else:\n",
    "    male_df = backup\n",
    "\n",
    "# T20 filter\n",
    "if 'info.match_type' in male_df.columns:\n",
    "    t20_df = male_df[male_df['info.match_type'] == 'T20']\n",
    "    print(\"After T20 filter:\", len(t20_df))\n",
    "else:\n",
    "    t20_df = male_df\n",
    "\n",
    "# 20 overs filter\n",
    "if 'info.overs' in t20_df.columns:\n",
    "    final_df = t20_df[t20_df['info.overs'] == 20]\n",
    "    print(\"After 20 overs filter:\", len(final_df))\n",
    "else:\n",
    "    final_df = t20_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffcb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
